{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2f5539",
   "metadata": {},
   "source": [
    "## Tools in Langchain\n",
    "1. How to create tools <br>\n",
    "2. How to use built-in tools and toolkit <br>\n",
    "3. How to use chat models to call tools <br>\n",
    "4. How to pass tool outputs to chat model <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670171c3",
   "metadata": {},
   "source": [
    "This @tool decoration is the simplest way to define a custom tool. The decorator uses the function name by default, but this can be overriden by passing a string as the first argument. Additionally, the decorator will use the function's docstring as the tool's description- so a docstring must be provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "484f2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = \"qwen/qwen3-32b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "053da8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## @tool decorator\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def division(a:int, b:int) -> int:\n",
    "    \"\"\"This function divides two numbers\"\"\"\n",
    "    return a/b\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e2a1e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division\n",
      "This function divides two numbers\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(division.name)\n",
    "print(division.description)\n",
    "print(division.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "27dd6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### async implementation\n",
    "\n",
    "@tool\n",
    "async def multiply(a:int, b:int) -> int:\n",
    "    \"\"\"This function multiplies 2 number\"\"\"\n",
    "    return a*b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9334a763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division\n",
      "This function divides two numbers\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(division.name)\n",
    "print(division.description)\n",
    "print(division.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6581d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "@tool\n",
    "def multiplyTwoNumbers(a: Annotated[int, \"First number\"], b: Annotated[int, \"second number\"]) -> int :\n",
    "    \"\"\"Multiply the two numbers\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8ebe283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiply the two numbers\n",
      "{'a': {'description': 'First number', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'second number', 'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(multiplyTwoNumbers.description)\n",
    "print(multiplyTwoNumbers.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd1ec8c",
   "metadata": {},
   "source": [
    "### Structured Tool\n",
    "The StructuredTool.from_function class method provides a bit more configurability then @tool decorator, without requiring much additional code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d570f02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiply called\n",
      "6\n",
      "Async multiplied called\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "def multiply(a: int, b: int) -> int :\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    print(\"Multiply called\")\n",
    "    return a*b\n",
    "\n",
    "async def asyncMultiply(a:int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers\"\"\"\n",
    "    print(\"Async multiplied called\")\n",
    "    return a*b\n",
    "\n",
    "calculator = StructuredTool.from_function(multiply, asyncMultiply)\n",
    "\n",
    "print(calculator.invoke({\"a\":2, \"b\":3}))\n",
    "print(await calculator.ainvoke({\"a\":2, \"b\":3}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e5b8b",
   "metadata": {},
   "source": [
    "### Inbuilt Tools\n",
    "\n",
    "https://docs.langchain.com/oss/python/integrations/tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff0922",
   "metadata": {},
   "source": [
    "### WikiPedia integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ffa4dc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n",
      "\n",
      "Page: Vector database\n",
      "Summary: A vector database, vector store or vector search engine is a database that uses the vector space model to store vectors\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "\n",
    "apiWrapper = WikipediaAPIWrapper(top_k_results=5, doc_content_chars_max=500)\n",
    "tool = WikipediaQueryRun(api_wrapper=apiWrapper)\n",
    "\n",
    "response = tool.invoke({\"query\": \"langchain\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a8398",
   "metadata": {},
   "source": [
    "### Duckduck go integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "98f0478e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 days ago · Today gold price in India for 24 karat gold is 124,741 rupees per 10 grams. Gold price in India for 22 karat gold is 114,346 rupees per 10 grams. Gold rate per tola (24 karat) is ₹145,495.45 - One tola is equal to 180 troy grains which is 11.6638038 grams. This Gold calculator uses the current Gold Rate Today Calculator rate today for India only. 21 hours ago · The price of gold in India today is ₹12,704 per gram for 24 karat gold, ₹11,645 per gram for 22 karat gold and ₹9,528 per gram for 18 karat gold (also called 999 gold). The current price of 24-carat gold in India is Rs 124,540 per 10 gms, which is gain of 1.28% from its previous close. Gold And Silver Rates Today in India : On March 28, 2025, the price of gold experienced a flat movement in the morning. Gold and silver prices today In India : Gold hit a ... What Is The Price Of 22kt, 24kt Gold Rates Today In India Across Key Cities On September 09?\n",
      "------\n",
      "snippet: The price of 1 gram gold rate today for 24 carat gold is Rs 12,368 per gm. Gold prices in India are often measured in 'tola,' a colloquial term referring to the price for every 10 grams of gold ., title: Today 's Gold Rate in India : Gold Price ... | The Financial Express, link: https://www.financialexpress.com/gold-rate-today/, snippet: The price of gold in India today is ₹12,791 per gram for 24 karat gold , ₹11,725 per gram for 22 karat gold and ₹9,593 per gram for 18 karat gold (also called 999 gold ). Gold has over the years been a perfect hedge against inflation., title: Gold Rate Today (26 November 2025), Gold Price in India, link: https://www.goodreturns.in/gold-rates/, snippet: Stay updated with the most accurate and live gold prices in India today . Samco provides real-time rates for 24 Karat, 22 Karat, and 18 Karat gold across major cities, helping you make well-informed decisions about your gold purchases and investments., title: Gold Price Today in India | Real-Time 24K & 22K Gold Rates, link: https://www.samco.in/gold-rates, snippet: The gold price in India today is affected by several factors.In this context, the currency which acts as the biggest determinant of the gold price in India today is the USD. If the value of USD goes up, gold rates tend to reflect a downward slope worldwide., title: Gold Rate in India - LIVE Price of 22 & 24 Carat Gold Today, link: https://groww.in/gold-rates/gold-rate-today-in-india\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "result = DuckDuckGoSearchResults()\n",
    "\n",
    "searchResponse = search.invoke(\"Price of gold in india today\")\n",
    "resultResponse = result.invoke(\"Price of gold in india today\")\n",
    "\n",
    "print(searchResponse)\n",
    "print(\"------\")\n",
    "print(resultResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89baf17",
   "metadata": {},
   "source": [
    "### call tool with my LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68956ceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to infer model provider for model='qwen/qwen3-32b', please specify model_provider directly.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m a*b\n\u001b[32m     17\u001b[39m tools = [wikiTool, searchTool, multiply]\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m llm = \u001b[43minit_chat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelName\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m llm_with_tools = llm.bind_tools(tools=tools)\n\u001b[32m     21\u001b[39m multiplyQuery = \u001b[33m\"\u001b[39m\u001b[33mWhat is 2*3\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AgenticAi/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:314\u001b[39m, in \u001b[36minit_chat_model\u001b[39m\u001b[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m     warnings.warn(\n\u001b[32m    307\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m has been set but no fields are configurable. Set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    308\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    309\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconfigurable.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    310\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_chat_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m    320\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AgenticAi/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:336\u001b[39m, in \u001b[36m_init_chat_model_helper\u001b[39m\u001b[34m(model, model_provider, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_chat_model_helper\u001b[39m(\n\u001b[32m    331\u001b[39m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    332\u001b[39m     *,\n\u001b[32m    333\u001b[39m     model_provider: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    334\u001b[39m     **kwargs: Any,\n\u001b[32m    335\u001b[39m ) -> BaseChatModel:\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     model, model_provider = \u001b[43m_parse_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_provider == \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    338\u001b[39m         _check_pkg(\u001b[33m\"\u001b[39m\u001b[33mlangchain_openai\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AgenticAi/.venv/lib/python3.12/site-packages/langchain/chat_models/base.py:514\u001b[39m, in \u001b[36m_parse_model\u001b[39m\u001b[34m(model, model_provider)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_provider:\n\u001b[32m    511\u001b[39m     msg = (\n\u001b[32m    512\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to infer model provider for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, please specify model_provider directly.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    515\u001b[39m model_provider = model_provider.replace(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m).lower()\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, model_provider\n",
      "\u001b[31mValueError\u001b[39m: Unable to infer model provider for model='qwen/qwen3-32b', please specify model_provider directly."
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun, DuckDuckGoSearchRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "wikiApi = WikipediaAPIWrapper(top_k_results=5, doc_content_chars_max=100)\n",
    "wikiTool = WikipediaQueryRun(api_wrapper=wikiApi)\n",
    "\n",
    "searchTool = DuckDuckGoSearchRun()\n",
    "\n",
    "@tool\n",
    "def multiply(a:int, b:int) -> int :\n",
    "    \"\"\"Mutliple two numbers\"\"\"\n",
    "    return a*b\n",
    "\n",
    "tools = [wikiTool, searchTool, multiply]\n",
    "llm = init_chat_model(modelName)\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "multiplyQuery = \"What is 2*3\"\n",
    "addQuery = \"What is 2+3\"\n",
    "todayNews = \"tell me top 5 news\"\n",
    "\n",
    "multiplyResponse = llm_with_tools.invoke(multiplyQuery)\n",
    "addResponse = llm_with_tools.invoke(addQuery)\n",
    "newsResponse = llm_with_tools.invoke(todayNews)\n",
    "\n",
    "print(\"--------\")\n",
    "print(multiplyResponse)\n",
    "print(multiplyResponse.tool_calls)\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "print(addResponse)\n",
    "print(addResponse.tool_calls)\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "print(newsResponse)\n",
    "print(newsResponse.tool_calls)\n",
    "\n",
    "toolNamesMap = {\"wikiPedia\": wikiTool,\"multiply\": multiply, \"duckduckgo_search\": searchTool}\n",
    "messages = []\n",
    "## Executing the tool calls\n",
    "for tool in multiplyResponse.tool_calls:\n",
    "    selected_tool = toolNamesMap[tool[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "print(\"--------\")\n",
    "print(messages)\n",
    "\n",
    "## lets modifying these for all responses\n",
    "allResponses = [newsResponse, addResponse, multiplyResponse]\n",
    "allReponseMesages = []\n",
    "for res in allResponses:\n",
    "    print(\"Res\",res)\n",
    "    for tool in res.tool_calls:\n",
    "        selected_tool = toolNamesMap[tool[\"name\"].lower()]\n",
    "        tool_msg = selected_tool.invoke(tool)\n",
    "        allReponseMesages.append(tool_msg)\n",
    "\n",
    "print(\"--------\")\n",
    "print(allReponseMesages)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8271c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='What is 2*3', additional_kwargs={}, response_metadata={}), ToolMessage(content='6', name='multiply', tool_call_id='e37vfwdqe')]\n",
      "content='The result of 2*3 is 6.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 48, 'total_tokens': 60, 'completion_time': 0.020089717, 'completion_tokens_details': None, 'prompt_time': 0.002282235, 'prompt_tokens_details': None, 'queue_time': 0.056015245, 'total_time': 0.022371952}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_4387d3edbb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--d80c3f66-0215-4484-b709-8abbc037b46f-0' usage_metadata={'input_tokens': 48, 'output_tokens': 12, 'total_tokens': 60}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "allMessages=[HumanMessage(multiplyQuery)]\n",
    "\n",
    "\n",
    "for tool in multiplyResponse.tool_calls:\n",
    "    selected_tool = toolNamesMap[tool[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool)\n",
    "    allMessages.append(tool_msg)\n",
    "\n",
    "print(allMessages)\n",
    "finalResponse = llm_with_tools.invoke(allMessages)\n",
    "print(finalResponse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a858d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'duckduckgo_search'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchTool.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedeae45",
   "metadata": {},
   "source": [
    "### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5294d3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model returned tool calls: [{'name': 'wikipedia', 'args': {'query': 'langchain'}, 'id': 'cc9r5kaps', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 2, 'b': 5}, 'id': '8exezeqar', 'type': 'tool_call'}]\n",
      "Final content: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into various applications, such as chatbots, virtual assistants, and more. It provides a set of tools and APIs that make it easier to work with LLMs, including tasks like text generation, question-answering, and text classification.\n",
      "\n",
      "As for the math problem, 2*5 equals 10.\n",
      "Final tool_calls: []\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "query = \"What is langchain and what is 2*5\"\n",
    "messages = [HumanMessage(content=query)]\n",
    "\n",
    "toolNamesMap = {\n",
    "    \"wikipedia\": wikiTool,\n",
    "    \"multiply\": multiply,\n",
    "    \"duckduckgo_search\": searchTool,\n",
    "}\n",
    "\n",
    "# ---- 1st call: model decides which tools to call ----\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "print(\"Model returned tool calls:\", ai_msg.tool_calls)\n",
    "\n",
    "messages.append(ai_msg)\n",
    "\n",
    "# ---- Run the tools and append ToolMessages ----\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    name = tool_call[\"name\"]\n",
    "    args = tool_call[\"args\"]\n",
    "    call_id = tool_call[\"id\"]\n",
    "\n",
    "    selected_tool = toolNamesMap[name.lower()]\n",
    "    tool_result = selected_tool.invoke(args)  # note: pass args, not the whole tool_call\n",
    "\n",
    "    # Normalize to string content\n",
    "    if hasattr(tool_result, \"content\"):\n",
    "        content = tool_result.content\n",
    "    else:\n",
    "        content = str(tool_result)\n",
    "\n",
    "    # IMPORTANT: only content + tool_call_id are required\n",
    "    messages.append(ToolMessage(content, tool_call_id=call_id))\n",
    "\n",
    "# ---- 2nd call: model should now answer using tool results ----\n",
    "final_response = llm_with_tools.invoke(messages)\n",
    "print(\"Final content:\", final_response.content)\n",
    "print(\"Final tool_calls:\", getattr(final_response, \"tool_calls\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee32732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
